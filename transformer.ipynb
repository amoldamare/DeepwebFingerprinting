{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Layer, Dense, TimeDistributed, Concatenate, InputSpec, Wrapper, RNN\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(ScaledDotProductAttention,self).__init__(**kwargs)\n",
    "    def call(self,x,mask=None):\n",
    "        \"\"\"\n",
    "            Attention(Q,K,V)=softmax(Q*K^T / sqrt(d_k))*V\n",
    "        \"\"\"\n",
    "        q,k,v=x\n",
    "        d_k=q.shape.as_list()[2]\n",
    "        weights=K.batch_dot(q,k,axes=[2,2])\n",
    "        \n",
    "        if mask is not None:\n",
    "            weights+=-1e10*(1-mask)\n",
    "        \n",
    "        weights=K.softmax(weights/np.sqrt(d_k))\n",
    "        output=K.batch_dot(weights,v)\n",
    "        return output,weights\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        super(ScaledDotProductAttention,self).build(input_shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self,h,**kwargs):\n",
    "        self.h=h\n",
    "        super(MultiHeadAttention,self).__init__(**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        d_k,d_v=input_shape[1][-1]\n",
    "        d_model=self.h*d_k\n",
    "        self._q_layers=[]\n",
    "        self._k_layers=[]\n",
    "        self._v_layers=[]\n",
    "        self.sdpa_layer=ScaledDotProductAttention()\n",
    "        self._output=TimeDistributed(Dense(d_model))\n",
    "        for _ in range(self.h):\n",
    "            self._q_layers.append(TimeDistributed(Dense(d_k,activation=\"relu\",use_bias=False)))\n",
    "            self._k_layers.append(TimeDistributed(Dense(d_k,activation=\"relu\",use_bias=False)))\n",
    "            self._v_layers.append(TimeDistributed(Dense(d_v,activation=\"relu\",use_bias=False)))\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x,mask=None):\n",
    "        \"\"\"\n",
    "            MultiHeadAttention(q,k,v)=concat(head_1,...head_h)*W_0\n",
    "            head_i=Attention(q*W_q_i,k*W_k_i,v*W_v_i)\n",
    "        \"\"\"\n",
    "        q,k,v=x\n",
    "        outputs=[]\n",
    "        attentions=[]\n",
    "        for i in range(self.h):\n",
    "            qi=self._q_layers[i](q)\n",
    "            ki=self._k_layers[i](k)\n",
    "            vi=self._v_layers[i](v)\n",
    "            output,attention=self.sdpa_layer([qi,ki,vi],mask=mask)\n",
    "            outputs.append(output)\n",
    "            attentions.append(attention)\n",
    "        \n",
    "        concatenated_outputs=Concatenate()(outputs)\n",
    "        concatenated_attentions=Concatenate()(attentions)\n",
    "        output=self._output(concatenated_outputs)\n",
    "        return [output,concatenated_attentions]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
