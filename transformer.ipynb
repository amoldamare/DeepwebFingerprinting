{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Layer, Dense, TimeDistributed, Concatenate, InputSpec, Wrapper, RNN,Conv1D,Lambda,Add\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(ScaledDotProductAttention,self).__init__(**kwargs)\n",
    "    def call(self,x,mask=None):\n",
    "        \"\"\"\n",
    "            Attention(Q,K,V)=softmax(Q*K^T / sqrt(d_k))*V\n",
    "        \"\"\"\n",
    "        q,k,v=x\n",
    "        d_k=q.shape.as_list()[2]\n",
    "        weights=K.batch_dot(q,k,axes=[2,2])\n",
    "        \n",
    "        if mask is not None:\n",
    "            weights+=-1e10*(1-mask)\n",
    "        \n",
    "        weights=K.softmax(weights/np.sqrt(d_k))\n",
    "        output=K.batch_dot(weights,v)\n",
    "        return output,weights\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        super(ScaledDotProductAttention,self).build(input_shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self,h,d_k,**kwargs):\n",
    "        self.h=h\n",
    "        self.d_k=d_k\n",
    "        self.d_v=d_k\n",
    "        self.d_model=self.h*d_k\n",
    "        self._q_layers=[]\n",
    "        self._k_layers=[]\n",
    "        self._v_layers=[]\n",
    "        self.sdpa_layer=ScaledDotProductAttention()\n",
    "        self._output=TimeDistributed(Dense(self.d_model))\n",
    "        for _ in range(self.h):\n",
    "            self._q_layers.append(TimeDistributed(Dense(self.d_k,activation=\"relu\",use_bias=False)))\n",
    "            self._k_layers.append(TimeDistributed(Dense(self.d_k,activation=\"relu\",use_bias=False)))\n",
    "            self._v_layers.append(TimeDistributed(Dense(self.d_v,activation=\"relu\",use_bias=False)))\n",
    "            \n",
    "        super(MultiHeadAttention,self).__init__(**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x,mask=None):\n",
    "        \"\"\"\n",
    "            MultiHeadAttention(q,k,v)=concat(head_1,...head_h)*W_0\n",
    "            head_i=Attention(q*W_q_i,k*W_k_i,v*W_v_i)\n",
    "        \"\"\"\n",
    "        q,k,v=x\n",
    "        outputs=[]\n",
    "        attentions=[]\n",
    "        for i in range(self.h):\n",
    "            qi=self._q_layers[i](q)\n",
    "            ki=self._k_layers[i](k)\n",
    "            vi=self._v_layers[i](v)\n",
    "            output,attention=self.sdpa_layer([qi,ki,vi],mask=mask)\n",
    "            outputs.append(output)\n",
    "            attentions.append(attention)\n",
    "        \n",
    "        concatenated_outputs=Concatenate()(outputs)\n",
    "        concatenated_attentions=Concatenate()(attentions)\n",
    "        output=self._output(concatenated_outputs)\n",
    "        return [output,concatenated_attentions]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(Layer):\n",
    "    def __init__(self,d_model=512,d_ff=2048,**kwargs):\n",
    "        self.d_model=d_model,\n",
    "        self.d_ff=d_ff\n",
    "        self.conv1=Conv1D(self.d_ff,kernel_size=1,activation='relu')\n",
    "        self.conv2=Conv1D(self.d_model,kernel_size=1)\n",
    "        super(PositionWiseFeedForward,self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        super(PositionWiseFeedForward,self).build(input_shape)\n",
    "        \n",
    "    def call(self,x):\n",
    "        temp_x=self.conv1(x)\n",
    "        return self.conv2(temp_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(LayerNormalization,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(name='normalization_weights',shape=(input_shape[-1],),initializer=Ones(),trainable=True)\n",
    "        self.b=self.add_weight(name='bias',shape=(input_shape[-1],),initializer=Zeros(),trainable=True)\n",
    "        super(LayerNormalization,self).build(input_shape)\n",
    "    \n",
    "    def call(self,x):\n",
    "        mean=K.mean(x,axis=-1)\n",
    "        std=K.std(x,axis=-1)\n",
    "        output=self.g*(x-mean)/(std+1e-8)+self.b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self,h=8,d_k=64,d_hidden=2048,**kwargs):\n",
    "        self.h=h\n",
    "        self.d_k=64\n",
    "        self.d_model=self.h*self.d_k\n",
    "        self.d_hidden=d_hidden\n",
    "        self.mha=MultiHeadAttention(self.h,self.d_k)\n",
    "        self.ln_1=LayerNormalization()\n",
    "        self.add_1=Add()\n",
    "        self.ffwd=PositionWiseFeedForward(d_model=self.d_model,d_ff=self.d_hidden)\n",
    "        self.ln_2=LayerNormalization()\n",
    "        self.add_2=Add()\n",
    "        super(EncoderLayer,self).__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def call(self,x):\n",
    "        y,_=self.mha([x,x,x])\n",
    "        y=self.add_1([x,y])\n",
    "        y=self.ln_1(y)\n",
    "        \n",
    "        x=self.ffwd(y)\n",
    "        x=self.add_2([x,y])\n",
    "        y=self.ln_2(x)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,h=8,d_k=64,d_hidden=2048,**kwargs):\n",
    "        self.h=h\n",
    "        self.d_k=64\n",
    "        self.d_model=self.h*self.d_k\n",
    "        self.d_hidden=d_hidden\n",
    "        self.mha_1=MultiHeadAttention(self.h,self.d_k)\n",
    "        self.ln_1=LayerNormalization()\n",
    "        self.add_1=Add()\n",
    "        self.mha_2=MultiHeadAttention(self.h,self.d_k)\n",
    "        self.ln_2=LayerNormalization()\n",
    "        self.add_2=Add()\n",
    "        self.ffwd=PositionWiseFeedForward(d_model=self.d_model,d_ff=self.d_hidden)\n",
    "        self.ln_3=LayerNormalization()\n",
    "        self.add_3=Add()\n",
    "        super(EncoderLayer,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,x,encoder_output):\n",
    "        \n",
    "        y,s_attn=self.mha_1([x,x,x])\n",
    "        y=self.add_1([x,y])\n",
    "        y=self.ln_1(y)\n",
    "        \n",
    "        x,enc_attn=self.mha_2([encoder_output,encoder_output,y])\n",
    "        x=self.add_2([x,y])\n",
    "        x=self.ln_2(x)\n",
    "        \n",
    "        y=self.ffwd(x)\n",
    "        y=self.add_3([x,y])\n",
    "        y=self.ln_3(y)\n",
    "        \n",
    "        return [y,s_attn,enc_attn]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self,n=6,h=8,d_k=64,d_hidden=2048,**kwargs):\n",
    "        self.n=n\n",
    "        self.h=h\n",
    "        self.d_k=d_k\n",
    "        self.d_hidden=d_hidden\n",
    "        self.layers=[]\n",
    "        for i in range(n):\n",
    "            layers.append(EncoderLayer(h=self.h,d_k=self.d_k,d_hidden=self.d_hidden))\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self,n=6,h=8,d_k=64,d_hidden=2048,**kwargs):\n",
    "        self.n=n\n",
    "        self.h=h\n",
    "        self.d_k=d_k\n",
    "        self.d_hidden=d_hidden\n",
    "        self.layers=[]\n",
    "        for i in range(n):\n",
    "            layers.append(DecoderLayer(h=self.h,d_k=self.d_k,d_hidden=self.d_hidden))\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x,encoder_output):\n",
    "        s_attns=[]\n",
    "        enc_attns=[]\n",
    "        for layer in self.layers:\n",
    "            x,s_attn,enc_attn=layer(x,encoder_output)\n",
    "            s_attns.append(s_attn)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return [x,s_attns,enc_attns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
